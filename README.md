# india-supreme-court-asr
Data pipeline that pulls audio and text data from Indian Supreme Court Hearings, and prepares the 
data for Automatic Speech Recognition (ASR) tasks. Also includes a notebook for an example of using
this dataset to finetune Whisper.

## Table of Contents
1. [Setup](#setup)
2. [Data Generation](#data-generation)
3. [Finetuning Model](#finetuning-model)

## Setup

It is recommended to set up a virtual environment and install the dependencies. 

```bash
# Create a virtual environment named 'venv'
python3 -m venv venv
# Activate virtual environment
source venv/bin/activate
# install dependencies
pip install -r requirements.txt
```

## Data Generation

To run the data generation script on the `case_data.csv` provided from the root directory of this
repository, which runs through the steps detailed in [Details.section](#data-generation/details)

```bash
python generate_labeled_data.py
```

The dataset generated by the script is uploaded on https://huggingface.co/datasets/divi212/india-supreme-court-audio

### Visualization

After generating the data, to visualize the dataset and get a qualititive evaluation of the data 
generation pipeline, run 

```bash
streamlit run data_viewer.py
``` 

This allows you to select an audio chunk to play and it displays the corresponding transcript. 
Additionally, the "Shuffle" button randomly chooses an audio chunk.

### Details

This will create two folders, `raw_data/` and `chunked_data/`.

In the `raw_data/` folder, you will see the a folder for every row in the csv with the files 
corresponding to each step in the process: 

#### Step 1: Data Loading

From `DataLoader` in `data_utils/data_loader.py`
(a) `raw_data/*/audio.mp3` - audio data downloaded directly from the `mp3 format link` column
(b) `raw_data/*/audio.wav` - audio data converted to .wav format
(c) `raw_data/*/transcript.pdf` - PDF transcript downloaded directly from the `Transcript Link` column. 
Note that these transcripts do not contain timestamps, and the processes that follow align the audio
and text.

#### Step 2: PDF to text 
From `PDFTranscriptProcessor` in `data_utils/pdf_to_text.py`
(a) `traw_data/*/ranscript.txt` - Cleaned text representation of the PDF

#### Step 3: Forced Alignment of the transcript and audio
`YouTubeCaptionAligner` in `data_utils/youtube_caption_aligner.py` is responsible for adding
timestamps on the transcript. It does this by forcibly aligning a transcript with the YouTube
captions and pulling the timestamps from the YouTube caption. 
(a) `raw_data/*/youtube_captions.json` - Raw captions pulled from the youtube_transcript_api
(b) `raw_data/*/aligned_transcript.json` - Chunked transcript with audio start and end times.

Note that `Aligner` in `data_utils/aligner.py` provides a base class for implementing additional 
alignment techniques.

#### Step 4: Chunking

The audio produced is too long to put directly into an ASR model. Whisper, for example, has a 
maximum audio length of 30 seconds. 

`Chunker` in `data_utils/chunker.py`, creates and saves audio snippets in subfolders within
`chunked_data/` corresponding to each row in the csv.

Additionally, the process creates `chunked_data/chunked_audio_metadata.csv` which has the audio file
name and the corresponding transcript in two columns. 

#### Step 5 (Optional): Export to hugging face

This can directly be fed into hugging face as I have done on
https://huggingface.co/datasets/divi212/india-supreme-court-audio.

Set `PUSH_TO_HUGGING_FACE = True` and the `HUGGING_FACE_REPO = <repo_name>` and then rerun the 
script to push to the repo.

## Finetuning Model

The notebook `whisper_finetuning.ipynb` is included which pulls the data from hugging face and 
finetunes the Whisper-tiny model.

In this notebook, there is an option to download the data directly from hugging face and optionally
storing it to disk. 
This notebook has been verified on a colab environment.

